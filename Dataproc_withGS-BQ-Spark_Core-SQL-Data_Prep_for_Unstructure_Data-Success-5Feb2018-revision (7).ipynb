{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instruction: Before run this code, please create table using cloud shell with \"bq mk nasa_dataset\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download raw data from a data source, and create a RDD from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-06-05 13:27:49--  https://s3.amazonaws.com/imcbucket/data/nasa.dat\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.169.69\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.169.69|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 205242368 (196M) [application/octet-stream]\n",
      "Saving to: ‘nasa.dat’\n",
      "\n",
      "nasa.dat            100%[===================>] 195.73M  24.6MB/s    in 7.6s    \n",
      "\n",
      "2019-06-05 13:27:56 (25.9 MB/s) - ‘nasa.dat’ saved [205242368/205242368]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://s3.amazonaws.com/imcbucket/data/nasa.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 205242368 Sep  6  2013 nasa.dat\r\n"
     ]
    }
   ],
   "source": [
    "! ls -l nasa*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://nasa.dat [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "- [1 files][195.7 MiB/195.7 MiB]                                                \n",
      "Operation completed over 1 objects/195.7 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp nasa.dat gs://aekanuntest/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245\r\n",
      "unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985\r\n",
      "199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085\r\n"
     ]
    }
   ],
   "source": [
    "! head -3 nasa.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_rdd = sc.textFile('gs://aekanuntest/data/nasa.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make pattern matching for extracting some information from raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import re\n",
    "from pyspark.sql import Row\n",
    "\n",
    "APACHE_ACCESS_LOG_PATTERN = '(\\S*) - - \\[(\\d{2})\\/(\\S*)\\/(\\d{4}):(\\d{2}):(\\d{2}):(\\d{2}) (\\S*)\\]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bejoindate(year,month,date):\n",
    "    s = '-'\n",
    "    seq = (year,month,date)\n",
    "    return s.join(seq)\n",
    "\n",
    "def bejointime(hour,minute,second):\n",
    "    s = ':'\n",
    "    seq = (hour,minute,second)\n",
    "    return s.join(seq)\n",
    "\n",
    "def bejoindatetime(date_name,time_name):\n",
    "    s = ' '\n",
    "    seq = (date_name,time_name)\n",
    "    return s.join(seq)\n",
    "\n",
    "def totimestamp(dt):\n",
    "    return time.mktime(datetime.datetime.\\\n",
    "    strptime(dt, \"%Y-%b-%d %H:%M:%S\").timetuple())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_apache_log_line(logline):\n",
    "    pattern = re.compile(APACHE_ACCESS_LOG_PATTERN)\n",
    "    result = pattern.match(logline)\n",
    "    if result is None:\n",
    "        return Row(\n",
    "        datetime_stamp = None,\n",
    "        ip_addr = None,\n",
    "        day_of_month = None,\n",
    "        month = None,\n",
    "        year = None,\n",
    "        hour = None,\n",
    "        minute = None,\n",
    "        second = None,\n",
    "        timezone = None\n",
    "        )\n",
    "    return Row(\n",
    "        #นำวันเดือนปีถูกแยกมาก่อนหน้านี้ กลับมา Join กันใหม่ใน Format ที่เหมาะสม\n",
    "        datetime_stamp = totimestamp(bejoindatetime(bejoindate(result.group(4).zfill(2),\\\n",
    "                                                               result.group(3),result.group(2).zfill(2)),\\\n",
    "                                                    bejointime(result.group(5),result.group(6),result.group(7)))),\n",
    "        ip_addr = result.group(1),\n",
    "        day_of_month = result.group(2),\n",
    "        month = result.group(3),\n",
    "        year = result.group(4),\n",
    "        hour = result.group(5),\n",
    "        minute = result.group(6),\n",
    "        second = result.group(7),\n",
    "        timezone = result.group(8)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_rdd = raw_rdd.map(parse_apache_log_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(datetime_stamp=804556801.0, day_of_month='01', hour='00', ip_addr='199.72.81.55', minute='00', month='Jul', second='01', timezone='-0400', year='1995')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a schema for the parsed data, and make data cleansing, and store data with their schema into the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = parsed_rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, DecimalType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_df = raw_df.withColumn('hour',raw_df['hour'].cast(IntegerType()))\\\n",
    ".withColumn('minute',raw_df['minute'].cast(IntegerType()))\\\n",
    ".withColumn('second',raw_df['second'].cast(IntegerType()))\\\n",
    ".withColumn('datetime_stamp',raw_df['datetime_stamp'].cast(TimestampType()))\\\n",
    ".dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|timezone|\n",
      "+--------+\n",
      "|   -0400|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_df.select(parsed_df['timezone']).distinct().orderBy(parsed_df['timezone']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datetime_stamp: timestamp (nullable = true)\n",
      " |-- day_of_month: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- ip_addr: string (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- second: integer (nullable = true)\n",
      " |-- timezone: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+----+--------------------+------+-----+------+--------+----+\n",
      "|     datetime_stamp|day_of_month|hour|             ip_addr|minute|month|second|timezone|year|\n",
      "+-------------------+------------+----+--------------------+------+-----+------+--------+----+\n",
      "|1995-07-01 00:00:01|          01|   0|        199.72.81.55|     0|  Jul|     1|   -0400|1995|\n",
      "|1995-07-01 00:00:06|          01|   0|unicomp6.unicomp.net|     0|  Jul|     6|   -0400|1995|\n",
      "|1995-07-01 00:00:09|          01|   0|      199.120.110.21|     0|  Jul|     9|   -0400|1995|\n",
      "|1995-07-01 00:00:11|          01|   0|  burger.letters.com|     0|  Jul|    11|   -0400|1995|\n",
      "|1995-07-01 00:00:11|          01|   0|      199.120.110.21|     0|  Jul|    11|   -0400|1995|\n",
      "|1995-07-01 00:00:12|          01|   0|  burger.letters.com|     0|  Jul|    12|   -0400|1995|\n",
      "|1995-07-01 00:00:12|          01|   0|  burger.letters.com|     0|  Jul|    12|   -0400|1995|\n",
      "|1995-07-01 00:00:12|          01|   0|     205.212.115.106|     0|  Jul|    12|   -0400|1995|\n",
      "|1995-07-01 00:00:13|          01|   0|         d104.aa.net|     0|  Jul|    13|   -0400|1995|\n",
      "|1995-07-01 00:00:13|          01|   0|      129.94.144.152|     0|  Jul|    13|   -0400|1995|\n",
      "+-------------------+------------+----+--------------------+------+-----+------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1891714"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading to BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the parsed data from the DataFrame into the BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Google Cloud Storage bucket for temporary BigQuery export data used\n",
    "# by the InputFormat. This assumes the Google Cloud Storage connector for\n",
    "# Hadoop is configured.\n",
    "bucket = sc._jsc.hadoopConfiguration().get('fs.gs.system.bucket')\n",
    "project = sc._jsc.hadoopConfiguration().get('fs.gs.project.id')\n",
    "input_directory = 'gs://{}/hadoop/tmp/bigquery/pyspark_input'.format(bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataproc-d148ab3e-1a71-4df8-85c6-19b9bc036446-us'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iot-class-feb2017'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://dataproc-d148ab3e-1a71-4df8-85c6-19b9bc036446-us/hadoop/tmp/bigquery/pyspark_input'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Output Parameters.\n",
    "output_dataset = 'nasa_dataset'\n",
    "output_table = 'nasa_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage data formatted as newline-delimited JSON in Google Cloud Storage.\n",
    "output_directory = 'gs://{}/hadoop/tmp/bigquery/pyspark_output'.format(bucket)\n",
    "#partitions = range(word_counts.getNumPartitions())\n",
    "output_files = output_directory + '/part-*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://dataproc-d148ab3e-1a71-4df8-85c6-19b9bc036446-us/hadoop/tmp/bigquery/pyspark_output/part-*'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_df.write.format('csv').save(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dataproc-d148ab3e-1a71-4df8-85c6-19b9bc036446-us/hadoop/tmp/bigquery/pyspark_output/part-00000-dd7400b3-bae8-47db-9de4-1731b4b225dd-c000.csv\r\n",
      "gs://dataproc-d148ab3e-1a71-4df8-85c6-19b9bc036446-us/hadoop/tmp/bigquery/pyspark_output/part-00001-dd7400b3-bae8-47db-9de4-1731b4b225dd-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls gs://dataproc-d148ab3e-1a71-4df8-85c6-19b9bc036446-us/hadoop/tmp/bigquery/pyspark_output/part-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "# Shell out to bq CLI to perform BigQuery import.\n",
    "subprocess.check_call(\n",
    "    'bq load --source_format=CSV  '\n",
    "    '--replace '\n",
    "    '--autodetect '\n",
    "    '{dataset}.{table} {files} '\n",
    "    'datetime_stamp:timestamp,day_of_month:INTEGER,hour:INTEGER,ip_addr:STRING,minute:INTEGER,month:STRING,second:INTEGER,timezone:INTEGER,year:INTEGER '\n",
    "    .format(\n",
    "        dataset=output_dataset, table=output_table, files=output_files\n",
    "    ).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually clean up the staging_directories, otherwise BigQuery\n",
    "# files will remain indefinitely.\n",
    "\n",
    "output_path = sc._jvm.org.apache.hadoop.fs.Path(output_directory)\n",
    "output_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(\n",
    "    output_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ls -l nasa.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subprocess.check_call(['bq', 'load', '--source_format=CSV', '--replace', '--autodetect','nasa_dataset.nasa_output', 'gs://dataproc-d148ab3e-1a71-4df8-85c6-19b9bc036446-us/hadoop/tmp/bigquery/pyspark_output/part-*', 'datetime_stamp:timestamp,day_of_month:INTEGER,hour:INTEGER,ip_addr:STRING,minute:INTEGER,month:STRING,second:INTEGER,timezone:INTEGER,year:INTEGER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
